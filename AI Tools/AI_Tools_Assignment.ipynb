{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab0ff21",
   "metadata": {},
   "source": [
    "# AI Tools Assignment: Mastering the AI Toolkit üõ†Ô∏èüß†\n",
    "\n",
    "This notebook covers theoretical and practical tasks on AI tools, frameworks, and ethical considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f11df1",
   "metadata": {},
   "source": [
    "## Part 1: Theoretical Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bbeedf",
   "metadata": {},
   "source": [
    "### Q1: Explain the primary differences between TensorFlow and PyTorch. When would you choose one over the other?\n",
    "\n",
    "**Answer:**\n",
    "- **TensorFlow** uses static computation graphs (with eager execution as an option), is widely used in production, and has strong deployment support (e.g., TensorFlow Lite, TensorFlow Serving).\n",
    "- **PyTorch** uses dynamic computation graphs, is more Pythonic, and is popular in research for its flexibility and ease of debugging.\n",
    "- **Choose TensorFlow** for production-ready, scalable solutions. **Choose PyTorch** for rapid prototyping, research, and when dynamic graph construction is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee0e6e",
   "metadata": {},
   "source": [
    "### Q2: Describe two use cases for Jupyter Notebooks in AI development.\n",
    "\n",
    "**Answer:**\n",
    "1. **Interactive Experimentation:** Test and visualize data preprocessing, model training, and evaluation in real-time.\n",
    "2. **Documentation & Sharing:** Combine code, results, and explanations for reproducible research and team collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c22b711",
   "metadata": {},
   "source": [
    "### Q3: How does spaCy enhance NLP tasks compared to basic Python string operations?\n",
    "\n",
    "**Answer:**\n",
    "- spaCy provides advanced NLP features like tokenization, part-of-speech tagging, named entity recognition, and dependency parsing.\n",
    "- It handles linguistic nuances and context, whereas basic string operations only perform simple pattern matching or splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81104686",
   "metadata": {},
   "source": [
    "### Comparative Analysis: Scikit-learn vs. TensorFlow\n",
    "\n",
    "| Feature | Scikit-learn | TensorFlow |\n",
    "|---------|--------------|------------|\n",
    "| Target Applications | Classical ML (e.g., regression, SVM) | Deep Learning (e.g., neural networks) |\n",
    "| Ease of Use | Beginner-friendly, simple API | Steeper learning curve, more flexible |\n",
    "| Community Support | Large, mature community | Large, active community, strong industry adoption |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a2fbb7",
   "metadata": {},
   "source": [
    "## Part 2: Practical Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221930b",
   "metadata": {},
   "source": [
    "### Task 1: Classical ML with Scikit-learn (Iris Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target)\n",
    "\n",
    "# Check for missing values\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc450b05",
   "metadata": {},
   "source": [
    "### Task 2: Deep Learning with TensorFlow (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train a CNN on MNIST using TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train[..., None]\n",
    "x_test = x_test[..., None]\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on 5 sample images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sample_idx = np.random.choice(len(x_test), 5, replace=False)\n",
    "sample_images = x_test[sample_idx]\n",
    "sample_labels = y_test[sample_idx]\n",
    "preds = model.predict(sample_images)\n",
    "\n",
    "for i in range(5):\n",
    "    plt.imshow(sample_images[i].reshape(28,28), cmap='gray')\n",
    "    plt.title(f'True: {sample_labels[i]}, Pred: {np.argmax(preds[i])}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2dd913",
   "metadata": {},
   "source": [
    "### Task 3: NLP with spaCy (Amazon Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10680b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition and Sentiment Analysis with spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "reviews = [\n",
    "    \"I love my new Apple iPhone! The camera is amazing.\",\n",
    "    \"The Samsung headphones broke after a week. Very disappointed.\"\n",
    "]\n",
    "\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    sentiment = 'positive' if any(word in review.lower() for word in ['love', 'amazing', 'great']) else 'negative'\n",
    "    print(f'Review: {review}')\n",
    "    print(f'Entities: {entities}')\n",
    "    print(f'Sentiment: {sentiment}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451bde7",
   "metadata": {},
   "source": [
    "## Part 3: Ethics & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabfb48d",
   "metadata": {},
   "source": [
    "### Ethical Considerations\n",
    "\n",
    "- **Potential Biases:**\n",
    "  - MNIST: May not generalize to non-digit images or different handwriting styles.\n",
    "  - Amazon Reviews: Sentiment rules may not capture sarcasm or cultural context.\n",
    "- **Mitigation:**\n",
    "  - Use tools like TensorFlow Fairness Indicators to evaluate model fairness.\n",
    "  - Use spaCy's rule-based systems to refine entity and sentiment extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac9a96",
   "metadata": {},
   "source": [
    "### Troubleshooting Challenge\n",
    "\n",
    "Below is a buggy TensorFlow script. Debug and fix errors (e.g., dimension mismatches, incorrect loss functions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed TensorFlow script for classification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Dummy data\n",
    "X = tf.random.normal((100, 20))\n",
    "y = tf.random.uniform((100,), maxval=2, dtype=tf.int32)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(20,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107c32e",
   "metadata": {},
   "source": [
    "## Bonus: Model Deployment\n",
    "\n",
    "Deploy your MNIST classifier using Streamlit or Flask. Example Streamlit code:\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('mnist_cnn.h5')\n",
    "uploaded_file = st.file_uploader('Upload an image', type=['png', 'jpg'])\n",
    "if uploaded_file:\n",
    "    # Preprocess and predict\n",
    "    st.image(uploaded_file)\n",
    "    # ...\n",
    "```\n",
    "\n",
    "Include a screenshot and live demo link in your report."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
